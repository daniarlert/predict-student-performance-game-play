{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook contains the code for the feature engineering, modeling and predictions. A big part of the code is based on [this notebook by Chris Deotte](https://www.kaggle.com/code/cdeotte/xgboost-baseline-0-680) since at first it didn't occur to me to load the dataset in chunks to avoid the memory issues that the main dataset size was causing.\n",
    "For the model, we use the [XGBoost](https://xgboost.readthedocs.io/en/latest/) library."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas scikit-learn xgboost matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import gc\n",
    "\n",
    "# import jo_wilder\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to files\n",
    "test_csv_path = \"./data/test.csv\"\n",
    "train_csv_path = \"./data/train.csv\"\n",
    "target_labels_csv = \"./data/train_labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load only session_id column\n",
    "tmp = pd.read_csv(train_csv_path, usecols=[0])\n",
    "tmp = tmp.groupby(\"session_id\")[\"session_id\"].agg(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate chunks and skips\n",
    "pieces = 20\n",
    "chunks = int(np.ceil(len(tmp) / pieces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads = []\n",
    "skips = [0]\n",
    "\n",
    "for k in range(pieces):\n",
    "    a = k * chunks\n",
    "    b = (k + 1) * chunks\n",
    "\n",
    "    if b > len(tmp):\n",
    "        b = len(tmp)\n",
    "\n",
    "    r = tmp.iloc[a:b].sum()\n",
    "    reads.append(r)\n",
    "    skips.append(skips[-1] + r)\n",
    "\n",
    "print(f\"pieces: {pieces} of sizes: {reads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_csv_path, nrows=reads[0])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = pd.read_csv(target_labels_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df[\"session\"] = target_df.session_id.apply(lambda x: int(x.split(\"_\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df[\"q\"] = target_df.session_id.apply(lambda x: int(x.split(\"_\")[-1][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df[\"correct\"] = target_df[\"correct\"].astype(\"int8\")\n",
    "target_df[\"q\"] = target_df[\"q\"].astype(\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    \"event_name\",\n",
    "    \"fqid\",\n",
    "    \"room_fqid\",\n",
    "    \"text\",\n",
    "    \"text_fqid\",\n",
    "]\n",
    "\n",
    "numerical_cols = [\n",
    "    \"elapsed_time\",\n",
    "    \"level\",\n",
    "    \"page\",\n",
    "    \"room_coor_x\",\n",
    "    \"room_coor_y\",\n",
    "    \"screen_coor_x\",\n",
    "    \"screen_coor_y\",\n",
    "    \"hover_duration\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_list = train_df[\"event_name\"].unique().tolist()\n",
    "event_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = train_df[\"text\"].unique().tolist()\n",
    "name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fqid_list = train_df[\"fqid\"].unique().tolist()\n",
    "fqid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_list = train_df[\"room_fqid\"].unique().tolist()\n",
    "room_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_cols = [\"session_id\", \"level_group\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(train_df):\n",
    "    dfs = []\n",
    "\n",
    "    agg_functions = {c: [\"mean\", \"std\", \"sum\", \"max\", \"min\"] for c in numerical_cols}\n",
    "\n",
    "    for c, funcs in agg_functions.items():\n",
    "        tmp = train_df.groupby(groupby_cols)[c].agg(funcs)\n",
    "        tmp.columns = [f\"{c}_{agg_name}\" for agg_name in funcs]\n",
    "        dfs.append(tmp)\n",
    "\n",
    "    for c in categorical_cols:\n",
    "        tmp = train_df.groupby(groupby_cols)[c].agg(\"nunique\")\n",
    "        tmp.name = f\"{tmp.name}_nunique\"\n",
    "        dfs.append(tmp)\n",
    "\n",
    "    for c in event_list:\n",
    "        train_df[c] = (train_df[\"event_name\"] == c).astype(np.int8)\n",
    "\n",
    "    for c in event_list:\n",
    "        tmp = train_df.groupby(groupby_cols).agg({c: \"sum\", \"elapsed_time\": \"sum\"})\n",
    "        tmp.rename(\n",
    "            columns={c: f\"{c}_sum\", \"elapsed_time\": f\"{c}_elapsed_time_sum\"},\n",
    "            inplace=True,\n",
    "        )\n",
    "        dfs.append(tmp)\n",
    "\n",
    "    for c in room_list:\n",
    "        train_df[c] = (train_df[\"room_fqid\"] == c).astype(np.int8)\n",
    "\n",
    "    for c in room_list:\n",
    "        tmp = train_df.groupby(groupby_cols)[c].agg(\"sum\")\n",
    "        tmp.name = f\"{tmp.name}_sum\"\n",
    "        dfs.append(tmp)\n",
    "\n",
    "    # Frequency encoding of fqid\n",
    "    fqid_counts = train_df[\"fqid\"].value_counts()\n",
    "    train_df[\"fqid_freq_encoded\"] = train_df[\"fqid\"].map(fqid_counts)\n",
    "\n",
    "    tmp = train_df.groupby(groupby_cols)[\"fqid_freq_encoded\"].agg(\n",
    "        [\"mean\", \"sum\", \"max\", \"min\"]\n",
    "    )\n",
    "    tmp.columns = [f\"fqid_freq_encoded_{agg_name}\" for agg_name in tmp.columns]\n",
    "    dfs.append(tmp)\n",
    "\n",
    "    train_df.drop(columns=[\"fqid\", \"fqid_freq_encoded\"], inplace=True)\n",
    "\n",
    "    # Frequency encoding of text\n",
    "    text_counts = train_df[\"text\"].value_counts()\n",
    "    train_df[\"text_freq_encoded\"] = train_df[\"text\"].map(text_counts)\n",
    "\n",
    "    tmp = train_df.groupby(groupby_cols)[\"text_freq_encoded\"].agg(\n",
    "        [\"mean\", \"sum\", \"max\", \"min\"]\n",
    "    )\n",
    "    tmp.columns = [f\"text_freq_encoded_{agg_name}\" for agg_name in tmp.columns]\n",
    "    dfs.append(tmp)\n",
    "\n",
    "    train_df.drop(columns=[\"text\", \"text_freq_encoded\"], inplace=True)\n",
    "\n",
    "    # Event frequency\n",
    "    event_freq = (\n",
    "        train_df.groupby(groupby_cols)[\"event_name\"]\n",
    "        .value_counts()\n",
    "        .unstack(fill_value=0)\n",
    "    )\n",
    "    event_freq.columns = [f\"{c}_freq\" for c in event_freq.columns]\n",
    "    dfs.append(event_freq)\n",
    "\n",
    "    # Session duration\n",
    "    session_duration = (\n",
    "        train_df.groupby(groupby_cols)[\"elapsed_time\"].max()\n",
    "        - train_df.groupby(groupby_cols)[\"elapsed_time\"].min()\n",
    "    )\n",
    "    session_duration.name = \"session_duration\"\n",
    "    dfs.append(session_duration)\n",
    "\n",
    "    # Event duration\n",
    "    event_duration = (\n",
    "        train_df.groupby(groupby_cols + [\"event_name\"])[\"elapsed_time\"].max()\n",
    "        - train_df.groupby(groupby_cols + [\"event_name\"])[\"elapsed_time\"].min()\n",
    "    )\n",
    "    event_duration = event_duration.unstack(fill_value=0)\n",
    "    event_duration.columns = [f\"{c}_duration\" for c in event_duration.columns]\n",
    "    dfs.append(event_duration)\n",
    "\n",
    "    # Event interval\n",
    "    train_df[\"event_interval\"] = train_df.groupby(groupby_cols)[\"elapsed_time\"].diff()\n",
    "    event_interval = train_df.groupby(groupby_cols)[\"event_interval\"].mean()\n",
    "    event_interval.name = \"event_interval\"\n",
    "    dfs.append(event_interval)\n",
    "\n",
    "    df = pd.concat(dfs, axis=1).fillna(-1)\n",
    "    df = df.reset_index().set_index(\"session_id\")\n",
    "\n",
    "    _ = gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process train_df in chunks\n",
    "all_chunks = []\n",
    "for k in range(pieces):\n",
    "    rows = 0\n",
    "    if k > 0:\n",
    "        rows = range(1, skips[k] + 1)\n",
    "        train_df = pd.read_csv(train_csv_path, skiprows=rows, nrows=reads[k])\n",
    "\n",
    "    df = feature_engineer(train_df)\n",
    "    all_chunks.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean memory\n",
    "del train_df\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all chunks\n",
    "df = pd.concat(all_chunks, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [c for c in df.columns if c != \"level_group\"]\n",
    "users = df.index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf = GroupKFold(n_splits=7)\n",
    "oof = pd.DataFrame(\n",
    "    data=np.zeros((len(users), 18)),\n",
    "    index=users,\n",
    ")\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (train_index, test_index) in enumerate(gkf.split(X=df, groups=df.index)):\n",
    "    print(f\"Fold {i + 1} => \", end=\"\")\n",
    "\n",
    "    xgb_params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"max_depth\": 4,\n",
    "        \"n_estimators\": 1000,\n",
    "        \"early_stopping_rounds\": 50,\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.4,\n",
    "        \"use_label_encoder\": False,\n",
    "    }\n",
    "\n",
    "    for t in range(1, 19):\n",
    "        if t <= 3:\n",
    "            grp = \"0-4\"\n",
    "        elif t <= 13:\n",
    "            grp = \"5-12\"\n",
    "        elif t <= 22:\n",
    "            grp = \"13-22\"\n",
    "\n",
    "        # Train data\n",
    "        train_x = df.iloc[train_index]\n",
    "        train_x = train_x.loc[train_x.level_group == grp]\n",
    "        train_users = train_x.index.values\n",
    "        train_y = target_df.loc[target_df.q == t].set_index(\"session\").loc[train_users]\n",
    "\n",
    "        # Valid data\n",
    "        valid_x = df.iloc[test_index]\n",
    "        valid_x = valid_x.loc[valid_x.level_group == grp]\n",
    "        valid_users = valid_x.index.values\n",
    "        valid_y = target_df.loc[target_df.q == t].set_index(\"session\").loc[valid_users]\n",
    "\n",
    "        # Train model\n",
    "        clf = XGBClassifier(**xgb_params)\n",
    "        clf.fit(\n",
    "            train_x[features].astype(\"float32\"),\n",
    "            train_y[\"correct\"],\n",
    "            eval_set=[(valid_x[features].astype(\"float32\"), valid_y[\"correct\"])],\n",
    "            verbose=0,\n",
    "        )\n",
    "        print(f\"{t}({clf.best_ntree_limit}), \", end=\"\")\n",
    "\n",
    "        # Save model and predict valid oof\n",
    "        models[f\"{grp}_{t}\"] = clf\n",
    "        oof.loc[valid_users, t - 1] = clf.predict_proba(\n",
    "            valid_x[features].astype(\"float32\")\n",
    "        )[:, 1]\n",
    "\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = oof.copy()\n",
    "for k in range(18):\n",
    "    # Get labels for each question\n",
    "    tmp = target_df.loc[target_df.q == k + 1].set_index(\"session\").loc[users]\n",
    "    true[k] = tmp.correct.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "thresholds = []\n",
    "\n",
    "best_score = 0\n",
    "best_threshold = 0\n",
    "\n",
    "for threshold in np.arange(0.4, 0.81, 0.01):\n",
    "    print(f\"{threshold:.02f}, \", end=\"\")\n",
    "    preds = (oof.values.reshape((-1)) > threshold).astype(\"int\")\n",
    "    m = f1_score(true.values.reshape((-1)), preds, average=\"macro\")\n",
    "    scores.append(m)\n",
    "    thresholds.append(threshold)\n",
    "    if m > best_score:\n",
    "        best_score = m\n",
    "        best_threshold = threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(thresholds, scores, \"-o\", color=\"blue\")\n",
    "plt.scatter([best_threshold], [best_score], color=\"blue\", s=300, alpha=1)\n",
    "plt.xlabel(\"Threshold\", size=14)\n",
    "plt.ylabel(\"Validation F1 Score\", size=14)\n",
    "plt.title(\n",
    "    f\"Threshold vs. F1_Score with Best F1_Score = {best_score:.3f} at Best Threshold = {best_threshold:.3}\",\n",
    "    size=18,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"When using optimal threshold...\")\n",
    "for k in range(18):\n",
    "    # Compute f1 score for each question\n",
    "    m = f1_score(\n",
    "        true[k].values, (oof[k].values > best_threshold).astype(\"int\"), average=\"macro\"\n",
    "    )\n",
    "    print(f\"Q{k}: F1 =\", m)\n",
    "\n",
    "# Compute overall F1 score\n",
    "m = f1_score(\n",
    "    true.values.reshape((-1)),\n",
    "    (oof.values.reshape((-1)) > best_threshold).astype(\"int\"),\n",
    "    average=\"macro\",\n",
    ")\n",
    "print(\"==> Overall F1 =\", m)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = jo_wilder.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory\n",
    "del target_df, df, oof, true\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limits = {\"0-4\": (1, 4), \"5-12\": (4, 14), \"13-22\": (14, 19)}\n",
    "\n",
    "for test, sample_submission in iter_test:\n",
    "    # Feature engineering\n",
    "    df = feature_engineer(test)\n",
    "\n",
    "    # Inference\n",
    "    grp = test.level_group.values[0]\n",
    "    a, b = limits[grp]\n",
    "    for t in range(a, b):\n",
    "        clf = models[f\"{grp}_{t}\"]\n",
    "        p = clf.predict_proba(df[features].astype(\"float32\"))[0, 1]\n",
    "        mask = sample_submission.session_id.str.contains(f\"q{t}\")\n",
    "        sample_submission.loc[mask, \"correct\"] = int(p > best_threshold)\n",
    "\n",
    "    env.predict(sample_submission)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"submission.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.correct.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
